---
title: 'Neural nets (almost) from scratch'
date: 2020-08-15
permalink: /posts/2020/08/blog-post-1/
tags:
  - artificial intelligence
  - neural nets
---

One thing that every data scienctist knows is how to deploy a neural net of its advanced versions using one of the big python libraries like tensorflow or pytorch. So why go through the pain of writing the model from (almost) scratch?  

Designing a ML model for a given dataset requires the programmer to go through certain steps, first to preprocess the data aand the second is tuning the parameters of the model. This article will delve into the second part and explain how learning the workings of a ML model might help us understand parameter tuning. I'll begin with a basic neural net with a few hidden layers.  

A basic neural net can be visualized as:  
![nn](/images/dense_nn_blog1.png)

This is the basic model architecture that will be followed in this post. As can be seen, a two dimensional input will be inputted to the model, and the model solves a classification problem, hence only a single input will decide if the input belongs to a community or not. 

## Defining the model architecture in python
A python list stores the characteristics of each layer, using dictionaries to store variables.
<script src="https://gist.github.com/sam14032000/67d9f4abbdcb50829a0131809cbb5c61.js"></script>
An additional variable mentioned here is the activation function. Rest assured, further sections will cover these functions. For now we move towards layer initiation. The input_dim variable specifies the length of input array, and a similar case is true ffor the output_dim. By intuition, the output of one layer will be the input of next layer, a symmetry which can easily be observed.
<script src="https://gist.github.com/sam14032000/6dbbc7ba9d1bc6fd3731efbbe723f70b.js"></script>
