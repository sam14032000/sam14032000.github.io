---
title: 'Neural nets (almost) from scratch'
date: 2020-08-15
permalink: /posts/2020/08/blog-post-1/
tags:
  - artificial intelligence
  - neural nets
---

One thing that every data scienctist knows is how to deploy a neural net of its advanced versions using one of the big python libraries like tensorflow or pytorch. So why go through the pain of writing the model from (almost) scratch?  

Designing a ML model for a given dataset requires the programmer to go through certain steps, first to preprocess the data aand the second is tuning the parameters of the model. This article will delve into the second part and explain how learning the workings of a ML model might help us understand parameter tuning. I'll begin with a basic neural net with a few hidden layers.  

A basic neural net can be visualized as:  
![nn](/images/dense_nn_blog1.png)

This is the basic model architecture that will be followed in this post. As can be seen, a two dimensional input will be inputted to the model, and the model solves a classification problem, hence only a single input will decide if the input belongs to a community or not.

## The basics
Forward propagation in a neural net is similar to a linear regression. This means that for each unit in each layer the weights of the regression equation have to be stored, to enable easy access. The parameters are stored in a navigable data structure (a list in this case, for simplicity) labelled param_cache, similarly the output values of the cells are stored in a seperate memory cache named memory (data structure same as that used in param_cache).  

## Defining the model architecture in python
A python list stores the characteristics of each layer, using dictionaries to store variables.  
<script src="https://gist.github.com/sam14032000/67d9f4abbdcb50829a0131809cbb5c61.js"></script>
An additional variable mentioned here is the activation function. Rest assured, further sections will cover these functions. For now we move towards layer initiation. The input_dim variable specifies the length of input array, and a similar case is true ffor the output_dim. By intuition, the output of one layer will be the input of next layer, a symmetry which can easily be observed.  
<script src="https://gist.github.com/sam14032000/6dbbc7ba9d1bc6fd3731efbbe723f70b.js"></script>
The above snippet initiates the memory cache which will store the variables for each layer, and as evident from the code initially the regression weights are initiated to random values to break symmetry, else the model will become a single linear regression without the ability to capture information.

## Activation functions
Activation functions are applied on functions often to bonud the output values within certain ranges, and to introcude non-linearity into the model. The functions implemented in the library are the 'ReLU' and 'sigmoid'. ReLU activation has often proven to be appropriate for hidden layers, since it allows selective transmission of information across the model. The final activation function can be 'sigmoid' or 'linear' based on whether the problem solves 'classification' or 'regression' respectively.
<script src="https://gist.github.com/sam14032000/5bb17aece2107e9f13990388ce194872.js"></script>

## Forward propagation
![forward_prop](/images/forward_prop.JPG)  
I'll introduce a few variable names here to be used for the rest of the post. Referring the above image, *z* is the layer input array, *A* is the inactivated layer output, *W* are the layer weights, *B* is the layer bias value (defined similarly to the constant term in linear regressions) and *g()* is the activation function. The subscript of the variables denote the layer index.  
<script src="https://gist.github.com/sam14032000/539884dca5cf06849bdb5b9d9617740c.js"></script>
The above snippet is the forward propagation through a single layer. 
<script src="https://gist.github.com/sam14032000/3ca61e8d70f939c8577edbf8c0b23242.js"></script>
full_forward_prop.py is a complete loop of forward propagation for one iteration, and it stores the layer outputs in the memory as it goes through the layers.

## Backward propagation
![back_prop](/images/back_prop.JPG)  
The back propagation of error in neural nets optimizes the parameter values of each layers based on the penalty incurred after one iteration of forward propagation.
<script src="https://gist.github.com/sam14032000/dde5cc76d444258db9dcc146063d0d64.js"></script>
The derivatives are calculated simply by chaining derivatives of the output values between layers.  
<script src="https://gist.github.com/sam14032000/52a25c62c7273de7dab37aafd8b4bbda.js"></script>
Above snippet is a complete loop of backward propagation accross the model. The first penalty in the model is introduced at the last output layer, when the error is calculated by measuring the distance between the output and the target.

## Learning phase
![nn_learn](/images/nn_learn.JPG)  
Once change required in the parameter is calculated, the parameters are updated according to the equation above, where "neta" is the learning rate chosen by the user. 
<script src="https://gist.github.com/sam14032000/c9a256092b864399be9eee7d9d0119df.js"></script>

## Wrapping it all up!
Finally all functions are packed up in sequence as:
<script src="https://gist.github.com/sam14032000/00849d6abb9cda7e7ded009641b9d062.js"></script>
The above snippet outputs the model parameters trained on a input dataset along with the true output values. Finally, the parameter cache returned by the above function is run on the test input data, which generates the output for the test data inputs.

## Scoring and accuracy
<script src="https://gist.github.com/sam14032000/03db51b7741b7352f8fc86d82a33ec3c.js"></script>
The scoring algorithms above are quite basic, and can work well for small or medium datasets. Test example for this library was implemented on the scikit make_moons data, which generates 1000 points to be classfied into two categories. This model with the architecture mentioned earlier gave an accuracy of 85%, a value which can be imrpoved with optimizers.

### Access the main library and the test exmaple [here](https://github.com/sam14032000/nn_from_scratch)

## Future Scope
I was inspired to write the blog after going through Andrew Ng's lecture series and especially the article on [neural nets](https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795) posted by Piotr Skalski on medium. After studying about neural nets by Andrew Ng I realized the importance of deep dives into AI models before implementation, and Piotr's article provided me a methodology to understand and implement the models. It will be my aim to implement every model I learn about similarly, along with the usage of optimizers wherever required.

### Thank you!
