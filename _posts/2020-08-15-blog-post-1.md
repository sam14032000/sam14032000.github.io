---
title: 'Neural nets (almost) from scratch'
date: 2020-08-15
permalink: /posts/2020/08/blog-post-1/
tags:
  - artificial intelligence
  - neural nets
---

One thing that every data scienctist knows is how to deploy a neural net of its advanced versions using one of the big python libraries like tensorflow or pytorch. So why go through the pain of writing the model from (almost) scratch?  

Designing a ML model for a given dataset requires the programmer to go through certain steps, first to preprocess the data aand the second is tuning the parameters of the model. This article will delve into the second part and explain how learning the workings of a ML model might help us understand parameter tuning. I'll begin with a basic neural net with a few hidden layers.  

A basic neural net can be visualized as:  
![nn](/images/dense_nn_blog1.png)

This is the basic model architecture that will be followed in this post. As can be seen, a two dimensional input will be inputted to the model, and the model solves a classification problem, hence only a single input will decide if the input belongs to a community or not.

## The basics
Forward propagation in a neural net is similar to a linear regression. This means that for each unit in each layer the weights of the regression equation have to be stored, to enable easy access. The parameters are stored in a navigable data structure (a list in this case, for simplicity) labelled param_cache, similarly the output values of the cells are stored in a seperate memory cache named memory (data structure same as that used in param_cache).  

## Defining the model architecture in python
A python list stores the characteristics of each layer, using dictionaries to store variables.  
<script src="https://gist.github.com/sam14032000/67d9f4abbdcb50829a0131809cbb5c61.js"></script>
An additional variable mentioned here is the activation function. Rest assured, further sections will cover these functions. For now we move towards layer initiation. The input_dim variable specifies the length of input array, and a similar case is true ffor the output_dim. By intuition, the output of one layer will be the input of next layer, a symmetry which can easily be observed.  
<script src="https://gist.github.com/sam14032000/6dbbc7ba9d1bc6fd3731efbbe723f70b.js"></script>
The above snippet initiates the memory cache which will store the variables for each layer, and as evident from the code initially the regression weights are initiated to random values to break symmetry, else the model will become a single linear regression without the ability to capture information.

## Forward propagation
![forward_prop](/images/forward_prop.JPG)  
I'll introduce a few variable names here to be used for the rest of the post. Referring the above image, *z* is the layer input array, *A* is the inactivated layer output, *W* are the layer weights, *B* is the layer bias value (defined similarly to the constant term in linear regressions) and *g()* is the activation function. The subscript of the variables denote the layer index.  
<script src="https://gist.github.com/sam14032000/539884dca5cf06849bdb5b9d9617740c.js"></script>
The above snippet is the forward propagation through a single layer. 
<script src="https://gist.github.com/sam14032000/3ca61e8d70f939c8577edbf8c0b23242.js"></script>
full_forward_prop.py is a complete loop of forward propagation for one iteration, and it stores the layer outputs in the memory as it goes through the layers.

## Backward propagation
![back_prop](/images/back_prop.JPG)  

